<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge. TabReX is a referenceless, property-driven framework for evaluating tabular generation via graph-based reasoning.">
  <meta name="keywords" content="TabReX, table evaluation, LLM, metrics, benchmark, tabular generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TabReX</title>

  

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body id="top">

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#top">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
      </a>
      <a class="navbar-item" href="#problem">The Problem</a>
      <a class="navbar-item" href="#approach">How TabReX Works</a>
      <a class="navbar-item" href="#qualitative">Qualitative Results</a>
      <a class="navbar-item" href="#benchmark">The Benchmark</a>
      <a class="navbar-item" href="#evaluation">Quantitative Results</a>
      
      <a class="navbar-item" href="#BibTeX">BibTeX</a>
    </div>

    

  </div>
</nav> -->


<section class="hero is-medium brand-hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TabReX:</h1>
          <p class="subtitle is-4">Tabular Referenceless eXplainable Evaluation</p>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://tejasanvekar.github.io/">Tejas Anvekar</a>,</span>
            <span class="author-block">Junha Park,</span>
            <span class="author-block"><a href="https://research.adobe.com/person/aparna-garimella/">Aparna Garimella</a>,</span>
            <span class="author-block"><a href="https://vgupta123.github.io/">Vivek Gupta</a></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./TabReX-input/TabReX.pdf"
                   class="external-link button is-normal is-rounded is-brand">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/CoRAL-ASU/TabReX"
                   class="external-link button is-normal is-rounded is-brand">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/jpark284/TabReX"
                   class="external-link button is-normal is-rounded is-brand">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="problem">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why is Evaluating Generated Tables Hard?</h2>
        <div class="box elevated content has-text-justified">
          <p>
            Evaluating tables generated by large language models is hard: text-only metrics (like ROUGE, BERTScore) miss the critical <strong>structural</strong> information of a table.
          </p>
          <p>
            On the other hand, reference-based metrics (like Exact Match) are too rigid. They fail to generalize across different tasks or schemas, penalizing tables that are factually correct but structured differently from a single "gold" reference.
          </p>
          <p>
            We need a metric that is <strong>referenceless</strong>, understands <strong>tabular structure</strong>, and is <strong>interpretable</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="approach">
  <div class="container is-max-desktop">
    <h2 class="title is-3">How TabReX Works: Evaluation as Graph Reasoning</h2>
    <div class="content has-text-justified">
      <p>
        TabReX approaches evaluation as <em>graph reasoning</em>. Instead of comparing to a reference table, we compare the generated table directly against the <strong>source text</strong>.
      </p>
      <p>
        We convert the source text and generated table into canonical KGs, align them with an LLM‑guided matcher, and score them with rubric rules for structure and factual fidelity. The result is interpretable scores, a tunable sensitivity–specificity trade‑off, and cell‑ or table‑level error traces for diagnosis.
      </p>
    </div>
    <div class="box elevated content has-text-centered">
      <img src="./static/images/TabReX.png" alt="TabReX pipeline overview" style="max-width: 100%;" />
      <p class="media-caption">
        The TabReX Pipeline: 1. Canonical KG conversion, 2. LLM-guided alignment, 3. Rubric-aware scoring with explainable traces.
      </p>
    </div>
  </div>
</section>


<section class="section" id="qualitative">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Finding Errors: What TabReX Sees</h2>
    <div class="box elevated content">
      <p>
        TabReX produces cell-level, table-level error traces that reveal where and why a generated table deviates from the source. These traces arise directly from <strong>graph alignment conflicts</strong> and rubric checks, helping users diagnose specific issues.
      </p>
      <p>
        We convert both the <strong>Source Text (g1)</strong> and the <strong>Generated Table (g2)</strong> into Knowledge Graphs, then align them to find mismatches. The graph triplets are in <code>[Row Header, Column Header, Cell Value]</code> format.
      </p>
    </div>

    <div class="box elevated content">
      <h4 class="title is-4">Example 1: Schema Mismatch</h4>
      <p><strong>Source Text:</strong> "The project involved Tejas, Junha, and Aparna, who are Researchers. Vivek is an Engineer."</p>
      
      <strong>Generated Table (g2)</strong>
      <table class="table is-bordered is-fullwidth">
        <thead><tr><th>Name</th><th>Position</th><th>Team</th></tr></thead>
        <tbody>
          <tr><td>Tejas</td><td>Researcher</td><td>AI</td></tr>
          <tr><td>Junha</td><td>Researcher</td><td>AI</td></tr>
          <tr><td>Aparna</td><td>Researcher</td><td>AI</td></tr>
          <tr><td>Vivek</td><td>Engineer</td><td>AI</td></tr>
        </tbody>
      </table>

      <div class="columns">
        <div class="column">
          <strong>Source Graph (g1) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Tejas, Position, Researcher]</code><br>
            <code>[Junha, Position, Researcher]</code><br>
            <code>[Aparna, Position, Researcher]</code><br>
            <code>[Vivek, Position, Engineer]</code>
          </div>
        </div>
        <div class="column">
          <strong>Table Graph (g2) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Tejas, Position, Researcher]</code>, <code>[Tejas, Team, AI]</code><br>
            <code>[Junha, Position, Researcher]</code>, <code>[Junha, Team, AI]</code><br>
            <code>[Aparna, Position, Researcher]</code>, <code>[Aparna, Team, AI]</code><br>
            <code>[Vivek, Position, Engineer]</code>, <code>[Vivek, Team, AI]</code>
          </div>
        </div>
      </div>

      <div class="columns">
        <div class="column">
          <strong>Alignment Result</strong>
          <div class="box has-background-danger-light" style="height: 100%;">
            <p class="has-text-danger-dark has-text-weight-bold">❌ Mismatch: Schema Mismatch</p>
            <p><strong>Alignment for 'Junha' (example):</strong></p>
            <ul style="margin-top: -10px;">
              <li><code style="font-size: 0.9rem;">[Junha/Junha, Position/Position, Researcher/Researcher]</code> (Match)</li>
              <li><code style="font-size: 0.9rem;">[Junha/Junha, -/Team, -/AI]</code> (Mismatch)</li>
            </ul>
            <p><strong>Reason:</strong> Relation <strong>'Team'</strong> in g2 is an <strong>Extra Column</strong> not found in g1.</p>
          </div>
        </div>
        <div class="column">
          <strong>Final Error Trace</strong>
          <table class="table is-bordered is-fullwidth">
            <thead><tr><th>Name</th><th>Position</th><th style="background-color: #ffe0e0; border: 2px solid red;">Team ❌</th></tr></thead>
            <tbody>
              <tr><td>Tejas</td><td>Researcher</td><td style="background-color: #ffe0e0;">AI</td></tr>
              <tr><td>Junha</td><td>Researcher</td><td style="background-color: #ffe0e0;">AI</td></tr>
              <tr><td>Aparna</td><td>Researcher</td><td style="background-color: #ffe0e0;">AI</td></tr>
              <tr><td>Vivek</td><td>Engineer</td><td style="background-color: #ffe0e0;">AI</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
    <div class="box elevated content">
      <h4 class="title is-4">Example 2: Entity Swap</h4>
      <p><strong>Source Text:</strong> "Sales for Q1 were $1000. Sales for Q2 were $1200."</p>

      <strong>Generated Table (g2)</strong>
      <table class="table is-bordered is-fullwidth">
        <thead><tr><th>Quarter</th><th>Sales</th></tr></thead>
        <tbody>
          <tr><td>Q1</td><td>$1200</td></tr>
          <tr><td>Q2</td><td>$1000</td></tr>
        </tbody>
      </table>

      <div class="columns">
        <div class="column">
          <strong>Source Graph (g1) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Q1, Sales, $1000]</code><br>
            <code>[Q2, Sales, $1200]</code>
          </div>
        </div>
        <div class="column">
          <strong>Table Graph (g2) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Q1, Sales, $1200]</code><br>
            <code>[Q2, Sales, $1000]</code>
          </div>
        </div>
      </div>

      <div class="columns">
        <div class="column">
          <strong>Alignment Result</strong>
          <div class="box has-background-danger-light" style="height: 100%;">
            <p class="has-text-danger-dark has-text-weight-bold">❌ Mismatch: Factual Mismatch (Value Swap)</p>
            <p><strong>Alignment for 'Q1':</strong></p>
            <ul style="margin-top: -10px;">
              <li><code style="font-size: 0.9rem;">[Q1/Q1, Sales/Sales, $1000/$1200]</code></li>
            </ul>
             <p><strong>Reason:</strong> Values for aligned triplet do not match. (Difference: $200)</p>
          </div>
        </div>
        <div class="column">
          <strong>Final Error Trace</strong>
          <table class="table is-bordered is-fullwidth">
            <thead><tr><th>Quarter</th><th>Sales</th></tr></thead>
            <tbody>
              <tr><td>Q1</td><td style="background-color: #ffe0e0; border: 2px solid red;">$1200 ❌</td></tr>
              <tr><td>Q2</td><td style="background-color: #ffe0e0; border: 2px solid red;">$1000 ❌</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>

    <div class="box elevated content">
      <h4 class="title is-4">Example 3: Aggregation Error</h4>
      <p><strong>Source Text:</strong> "Team A has 3 members. Team B has 4 members. Total members: 7."</p>
      
      <strong>Generated Table (g2)</strong>
      <table class="table is-bordered is-fullwidth">
        <thead><tr><th>Team</th><th>Members</th></tr></thead>
        <tbody>
          <tr><td>Team A</td><td>3</td></tr>
          <tr><td>Team B</td><td>4</td></tr>
          <tr><td>Total</td><td>12</td></tr>
        </tbody>
      </table>
      
      <div class="columns">
        <div class="column">
          <strong>Source Graph (g1) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Team A, Members, 3]</code><br>
            <code>[Team B, Members, 4]</code><br>
            <code>[Total, Members, 7]</code>
          </div>
        </div>
        <div class="column">
          <strong>Table Graph (g2) Triplets</strong>
          <div class="box" style="font-size: 0.9rem; background-color: #fafafa; height: 100%;">
            <code>[Team A, Members, 3]</code><br>
            <code>[Team B, Members, 4]</code><br>
            <code>[Total, Members, 12]</code>
          </div>
        </div>
      </div>

      <div class="columns">
        <div class="column">
          <strong>Alignment Result (Rubric Check)</strong>
          <div class="box has-background-danger-light" style="height: 100%;">
            <p class="has-text-danger-dark has-text-weight-bold">❌ Mismatch: Aggregation Error</p>
             <p><strong>Alignment for 'Total':</strong></p>
            <ul style="margin-top: -10px;">
              <li><code style="font-size: 0.9rem;">[Total/Total, Members/Members, 7/12]</code></li>
            </ul>
            <p><strong>Reason:</strong> Aggregation rubric failed. Expected '7' (from g1 or <code>SUM(3,4)</code>), but g2 has '12'.</p>
          </div>
        </div>
        <div class="column">
          <strong>Final Error Trace</strong>
          <table class="table is-bordered is-fullwidth">
            <thead><tr><th>Team</th><th>Members</th></tr></thead>
            <tbody>
              <tr><td>Team A</td><td>3</td></tr>
              <tr><td>Team B</td><td>4</td></tr>
              <tr><td>Total</td><td style="background-color: #ffe0e0; border: 2px solid red;">12 ❌</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="benchmark">
  <div class="container is-max-desktop">
    <h2 class="title is-3">How Can We Reliably Test a Metric?</h2>
    <div class="content has-text-justified">
      <p>
        <strong>TabReX‑Bench</strong> stress‑tests metric robustness and generalization. It spans multiple domains and planner‑driven perturbations across three difficulty tiers, enabling controlled, harder‑case analyses.
      </p>
      <div class="box elevated content has-text-centered">
        <div class="level">
          <div class="level-item has-text-centered">
            <div>
              <p class="heading">Domains</p>
              <p class="title">6</p>
            </div>
          </div>
          <div class="level-item has-text-centered">
            <div>
              <p class="heading">Perturbation Types</p>
              <p class="title">12</p>
            </div>
          </div>
          <div class="level-item has-text-centered">
            <div>
              <p class="heading">Difficulty Tiers</p>
              <p class="title">3</p>
            </div>
          </div>
        </div>
      </div>
      <p>
        We define two complementary perturbation groups: <em>Data‑Preserving</em> (Group&nbsp;0) alters layout or presentation (e.g., row/header reordering, unit conversion, paraphrasing) without changing factual content; <em>Data‑Altering</em> (Group&nbsp;1) introduces semantic modifications such as adding or deleting rows/columns, swapping numeric values, or injecting noise and misspellings. Each group is further stratified into three difficulty tiers (<em>Easy</em>, <em>Medium</em>, <em>Hard</em>), supporting controlled analyses of metric robustness as perturbation severity increases.
      </p>
    </div>
    <div class="box elevated content" style="margin-top: 2rem;">
            <h4 class="title is-4 has-text-centered">Benchmark Composition by Dataset</h4>
            <div class="table-container">
              <table class="table is-bordered is-striped is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Dataset</th>
                    <th># of Tables</th>
                    <th># Perturb / Table</th>
                    <th>Total Tables</th>
                    <th>Avg Row</th>
                    <th>Avg Col</th>
                    <th>Avg Cell</th>
                    <th>Avg Tokens</th>
                    <th>Avg Num</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong>FinQA</strong></td>
                    <td>150</td>
                    <td>12</td>
                    <td>1950</td>
                    <td>5.55</td>
                    <td>2.47</td>
                    <td>13.22</td>
                    <td>119.5</td>
                    <td>33.55</td>
                  </tr>
                  <tr>
                    <td><strong>HiTabQA</strong></td>
                    <td>150</td>
                    <td>12</td>
                    <td>1950</td>
                    <td>20.08</td>
                    <td>5.60</td>
                    <td>115.1</td>
                    <td>434.8</td>
                    <td>102.7</td>
                  </tr>
                  <tr>
                    <td><strong>ToTTo</strong></td>
                    <td>150</td>
                    <td>12</td>
                    <td>1950</td>
                    <td>24.97</td>
                    <td>5.49</td>
                    <td>142.2</td>
                    <td>361.3</td>
                    <td>69.63</td>
                  </tr>
                  <tr>
                    <td><strong>OpenML med</strong></td>
                    <td>10</td>
                    <td>12</td>
                    <td>120</td>
                    <td>4.20</td>
                    <td>11.58</td>
                    <td>47.94</td>
                    <td>210.9</td>
                    <td>23.80</td>
                  </tr>
                  <tr>
                    <td><strong>MIMIC-IV</strong></td>
                    <td>100</td>
                    <td>12</td>
                    <td>1200</td>
                    <td>10.58</td>
                    <td>3.94</td>
                    <td>40.84</td>
                    <td>153.5</td>
                    <td>26.29</td>
                  </tr>
                  <tr>
                    <td><strong>RotoWire</strong></td>
                    <td>150</td>
                    <td>12</td>
                    <td>1950</td>
                    <td>10.18</td>
                    <td>5.86</td>
                    <td>59.50</td>
                    <td>146.5</td>
                    <td>14.33</td>
                  </tr>
                </tbody>
                <tfoot>
                  <tr style="background-color: #f0f0f0;">
                    <td><strong>Total</strong></td>
                    <td><strong>710</strong></td>
                    <td>-</td>
                    <td><strong>9120</strong></td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                    <td>-</td>
                  </tr>
                </tfoot>
              </table>
            </div>
          </div>
          <div class="box elevated content has-text-centered">
            <img src="./static/images/TabRex-Bench.png" alt="TabReX‑Bench composition and protocol" style="max-width: 100%;" />
            <p class="media-caption">Benchmark overview — domains, perturbations, and difficulty tiers for robust evaluation.</p>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="evaluation">
  <div class="container is-max-desktop">
    <h2 class="title is-3">How Well Does TabReX Perform?</h2>
    <div class="box elevated content has-text-centered">
      <h3 class="title is-4">Alignment with Human Judgment on TabReX‑Bench</h3>
      
      <div class="box content has-background-info-light" style="margin-bottom: 1.25rem;">
        <p class="has-text-weight-bold is-size-5 has-text-centered">
          TabReX outperforms all traditional and referenceless LLM‑based metrics in aligning with expert rankings on TabReX‑Bench.
        </p>
      </div>
      <table class="table is-striped is-hoverable is-fullwidth">
        <thead>
          <tr>
            <th>Metric</th>
            <th>&rho;<sub>S</sub> &uparrow;</th>
            <th>&tau;<sub>K</sub> &uparrow;</th>
            <th>&tau;<sub>w</sub> &uparrow;</th>
            <th>RBO &uparrow;</th>
            <th>&zeta;<sub>F</sub> &downarrow;</th>
            <th>&pi;<sub>t</sub> &downarrow;</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td colspan="7" style="background-color:#FCFBD7; text-align:center;"><em>Non-LLM Based (w/ Ref)</em></td>
          </tr>
          <tr>
            <td><strong>EM</strong></td><td>45.88</td><td>39.38</td><td>39.51</td><td>43.33</td><td>47.49</td><td>58.40</td>
          </tr>
          <tr>
            <td><strong>chrF</strong></td><td>41.76</td><td>34.55</td><td>31.61</td><td>39.39</td><td>49.26</td><td>01.64</td>
          </tr>
          <tr>
            <td><strong>ROUGE-L</strong></td><td>31.18</td><td>26.69</td><td>22.56</td><td>37.65</td><td>55.94</td><td>01.97</td>
          </tr>
          <tr>
            <td><strong>BLUERT</strong></td><td>44.66</td><td>37.64</td><td>36.09</td><td>39.57</td><td>48.09</td><td>00.77</td>
          </tr>
          <tr>
            <td><strong>BERTScore</strong></td><td>36.21</td><td>30.66</td><td>27.96</td><td>38.11</td><td>53.25</td><td>00.92</td>
          </tr>
          <tr>
            <td><strong>H-Score</strong></td><td><strong>56.87</strong></td><td><strong>47.97</strong></td><td><strong>51.73</strong></td><td><strong>41.11</strong></td><td><strong>40.02</strong></td><td><strong>00.99</strong></td>
          </tr>
          <tr>
            <td colspan="7" style="background-color:#D7FFFE; text-align:center;"><em>LLM-Based (w/ Ref)</em></td>
          </tr>
          <tr>
            <td><strong>P-Score</strong></td><td>49.24</td><td>40.00</td><td>37.43</td><td>40.73</td><td>43.93</td><td>07.39</td>
          </tr>
          <tr>
            <td><strong>TabEval</strong></td><td>49.01</td><td>39.22</td><td>34.21</td><td>41.11</td><td>43.06</td><td>00.63</td>
          </tr>
          <tr>
            <td><strong>TabXEval</strong></td><td><strong>80.27</strong></td><td><strong>72.37</strong></td><td><strong>66.87</strong></td><td><strong>47.54</strong></td><td><strong>20.94</strong></td><td>45.33</td>
          </tr>
          <tr>
            <td colspan="7" style="background-color:#DEFCDE; text-align:center;"><em>(w/o Ref)</em></td>
          </tr>
          <tr>
            <td><strong>QuestEval</strong></td><td>62.93</td><td>52.29</td><td>51.71</td><td>42.70</td><td>35.04</td><td>03.03</td>
          </tr>
          <tr>
            <td><strong>TabReX (Ours)</strong></td><td><strong>74.51</strong></td><td><strong>64.24</strong></td><td><strong>62.28</strong></td><td><strong>44.85</strong></td><td><strong>27.01</strong></td><td>13.59</td>
          </tr>
        </tbody>
      </table>
      <p class="is-size-7 has-text-grey">
        Higher values of Spearman’s rank correlation (&rho;<sub>S</sub>), Kendall’s tau (&tau;<sub>K</sub>), weighted Kendall’s tau (&tau;<sub>w</sub>), and Rank‑Biased Overlap (RBO) indicate stronger monotonic and positional agreement with human orderings (&uparrow;), while lower values of Spearman’s footrule distance (&zeta;<sub>F</sub>) and tie ratio (&pi;<sub>t</sub>) denote better rank stability and finer discriminative resolution (&downarrow;).
      </p>
    </div>

    <div class="box elevated content has-text-centered" style="margin-top: 2rem;">
      <h3 class="title is-4">Do Ensembles Close the Gap?</h3>
      <p class="has-text-justified">
        Ensembles help but don’t close the gap. The best variant (LLM, harmonic) reaches &rho;<sub>S</sub>=0.56 and &tau;<sub>K</sub>=0.47, still behind TabReX (&rho;<sub>S</sub>=0.75, &tau;<sub>K</sub>=0.64) and with higher rank dispersion—simple averaging or harmonic averaging can’t match TabReX’s targeted, referenceless graph reasoning.
      </p>
      <div class="table-container">
        <table class="table is-bordered is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Metric</th>
              <th>&rho;<sub>S</sub> &uparrow;</th>
              <th>&tau;<sub>K</sub> &uparrow;</th>
              <th>&tau;<sub>w</sub> &uparrow;</th>
              <th>RBO &uparrow;</th>
              <th>&zeta;<sub>F</sub> &downarrow;</th>
              <th>&pi;<sub>t</sub> &downarrow;</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="7" style="background-color:#F1F8E9; text-align:center;"><em>Ensemble Baselines</em></td>
            </tr>
            <tr>
              <td><strong>Lex‑Emb (M)</strong></td><td>38.43</td><td>32.65</td><td>30.17</td><td>38.52</td><td>52.15</td><td>00.49</td>
            </tr>
            <tr>
              <td><strong>Lex‑Emb (H)</strong></td><td>29.80</td><td>24.00</td><td>19.68</td><td>37.65</td><td>55.04</td><td>00.63</td>
            </tr>
            <tr>
              <td><strong>LLM (M)</strong></td><td>48.49</td><td>39.21</td><td>36.94</td><td>40.56</td><td>44.38</td><td>00.42</td>
            </tr>
            <tr>
              <td><strong>LLM (H)</strong></td><td>56.00</td><td>46.93</td><td>50.64</td><td>40.95</td><td>40.63</td><td>00.42</td>
            </tr>
            <tr>
              <td><strong>Hybrid (M)</strong></td><td>32.04</td><td>24.94</td><td>20.29</td><td>37.03</td><td>51.51</td><td>01.13</td>
            </tr>
            <tr>
              <td><strong>Hybrid (H)</strong></td><td>54.03</td><td>42.71</td><td>32.61</td><td>42.31</td><td>40.11</td><td>01.13</td>
            </tr>
            <tr style="background-color: #d1f7d1; border: 2px solid #006400;">
              <td><strong>TabReX (Ours)</strong></td><td><strong>74.51</strong></td><td><strong>64.24</strong></td><td><strong>62.28</strong></td><td><strong>44.85</strong></td><td><strong>27.01</strong></td><td>13.59</td>
            </tr>
          </tbody>
        </table>
      </div>
      <div class="media-caption has-text-left">
        <p>
          Ensembles combine metric families using either simple Mean (M) or Harmonic (H) aggregation:
        </p>
        <ul style="margin-top: 0.25rem; margin-bottom: 0.25rem;">
          <li><strong>Lex‑Emb</strong> (lexical + embedding): EM, ROUGE‑L, BERTScore, BLEURT, chrF</li>
          <li><strong>LLM</strong> (LLM‑based): P‑Score, H‑Score</li>
          <li><strong>Hybrid</strong> (reference‑based + referenceless): TabXEval, QuestEval</li>
        </ul>
        <p>
          All ensemble variants fall short of TabReX, which achieves the highest correlation with expert rankings and better rank stability.
        </p>
      </div>
    </div>

    <div class="box elevated content has-text-centered" style="margin-top: 2rem;">
      <h3 class="title is-4">Correlation on Real-World Text-to-Table Generation</h3>
      <p class="has-text-justified">
        Beyond TabReX‑Bench, we measure correlation with expert rankings on a real‑world text‑to‑table dataset. TabReX again achieves the highest alignment across correlation metrics, outperforming reference‑based and referenceless baselines.
      </p>
      <div class="table-container">
        <table class="table is-bordered is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Metric</th>
              <th>&rho;<sub>S</sub> &uparrow;</th>
              <th>&tau;<sub>K</sub> &uparrow;</th>
              <th>RBO &uparrow;</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td colspan="4" style="background-color:#FCFBD7; text-align:center;"><em>Standard Metrics (w/ Ref)</em></td>
            </tr>
            <tr>
              <td>EM</td><td>-0.01</td><td>0.01</td><td>0.33</td>
            </tr>
            <tr>
              <td>ROUGE-L</td><td>0.33</td><td>0.25</td><td>0.29</td>
            </tr>
            <tr>
              <td>BERTScore</td><td>0.26</td><td>0.19</td><td>0.38</td>
            </tr>
            <tr>
              <td>BLEURT</td><td>0.29</td><td>0.20</td><td>0.39</td>
            </tr>
            <tr>
              <td>CHRF</td><td>0.25</td><td>0.19</td><td>0.36</td>
            </tr>
            <tr>
              <td colspan="4" style="background-color:#D7FFFE; text-align:center;"><em>LLM-Based (w/ Ref)</em></td>
            </tr>
            <tr>
              <td>TabEval</td><td>0.25</td><td>0.19</td><td>0.36</td>
            </tr>
            <tr>
              <td>TabXEval</td><td>0.24</td><td>0.17</td><td>0.37</td>
            </tr>
            <tr>
              <td colspan="4" style="background-color:#DEFCDE; text-align:center;"><em>(w/o Ref)</em></td>
            </tr>
            <tr>
              <td>QuestEval</td><td>0.28</td><td>0.20</td><td>0.39</td>
            </tr>
            <tr style="background-color: #d1f7d1; border: 2px solid #006400;">
              <td><strong>TabReX (Ours)</strong></td>
              <td><strong>0.39</strong></td>
              <td><strong>0.30</strong></td>
              <td><strong>0.41</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    
    <div class="box elevated content has-text-centered" style="margin-top: 2rem;">
      <h3 class="title is-4">Sensitivity–Specificity Under Stress</h3>
      <p class="has-text-justified">
        A robust evaluation metric must remain reliable not only in standard (easy) settings but also under hard perturbations—tables with subtle misalignments, semantic shifts, or fine-grained numeric errors. We use <strong>TabReX-Bench</strong> to sample both easy and hard cases and compute true-positive (sensitivity) and true-negative (specificity) rates.
      </p>
      <img src="./static/images/sens_vs_spec.png" alt="Sensitivity vs specificity trade-off" style="width: 100%; max-width: 900px; height: auto;" />
      <p class="media-caption">
        <strong>Metric Movements Across Difficulty Levels:</strong> Arrows show each metric’s shift from easy (blue) to hard (red) perturbations. The green region denotes the balanced ideal zone, and the dashed diagonal marks the optimal trade-off. <strong>TabReX stays near this zone, maintaining the right direction even for hard examples.</strong>
      </p>
    </div>
    <div class="box elevated content has-text-centered" style="margin-top: 2rem;">
      <h3 class="title is-4">Model and Prompt Analysis</h3>
      <p class="has-text-justified">
        TabReX's rubric-aware scoring enables coarse to fine-grained comparisons across <em>models</em> (e.g., Gemma 27B vs. 4B) and <em>prompting strategies</em> (e.g., Zero-Shot, Chain-of-Thought, Map&Make), measured at both <em>cell-level</em> and <em>table-level</em> granularity.
      </p>
      <img src="./static/images/alignment_models_prompts.png" alt="Model-vs-prompt alignment analysis" style="width: 100%; height: auto;" />
      
      <div class="content has-text-left" style="margin-top: 1rem;">
        <p class="media-caption" style="font-style: italic;">
          <strong>Rubric-wise alignment across models and prompting strategies:</strong>
          The top row shows cell-level agreement, while the bottom row shows table-level agreement.
        </p>

        <p><strong>Key Insights:</strong></p>
        <ol style="margin-left: 2em; margin-top: -10px; font-size: 0.95rem;">
          <li>
            <strong>Model Size:</strong> Larger models (Gemma 27B) show clear gains in local, fine-grained (cell-level) fidelity but not necessarily global (table-level) coherence.
          </li>
          <li>
            <strong>Reasoning Style:</strong> Reasoning-oriented ("Thinking") variants improve precision on numeric/structural dimensions but can reduce semantic coverage, favoring accuracy over breadth.
          </li>
          <li>
            <strong>Prompt Design:</strong> The prompt strategy (especially Map&Make) contributes as much as model scale to achieving a balanced alignment across all rubric dimensions.
          </li>
        </ol>
        <p>
          These results illustrate how a referenceless, explainable evaluation metric like TabReX can reveal the strengths and weaknesses of models and prompting strategies across hierarchical levels.
        </p>
      </div>
    </div>
  </div>
</section>



 
<a class="back-to-top button is-link is-light is-rounded" href="#top" aria-label="Back to top">
  <span class="icon"><i class="fas fa-arrow-up"></i></span>
  <span>Top</span>
  </a>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <div class="box elevated">
      <pre><code></code></pre>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./TabReX-input/TabReX.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>Page source code was adapted from <a href="https://nerfies.github.io/" target="_blank" rel="noopener">here</a>.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
